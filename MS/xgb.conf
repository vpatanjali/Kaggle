# General Parameters, see comment for each definition
# can be gbtree or gblinear
booster = gbtree 
# choose logistic regression loss function for binary classification
objective = multi:softprob
eval_metric = mlogloss
#silent = 1
num_class = 9

#max_iterations = 250

# Tree Booster Parameters
# step size shrinkage
#eta = 0.1 
# minimum loss reduction required to make a further partition
gamma = 1.0 
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 4 
# maximum depth of a tree
max_depth = 10 

subsample = 0.9

colsample_bytree = 0.8

# Task Parameters
# the number of round to do boosting
num_round = 50
# 0 means do not save any model except the final round model
save_period = 0 
# The path of training data
data = "/home/patanjali/Kaggle/Data/MS/dev.libsvm" 
# The path of validation data, used to monitor training process, here [test] sets name of the validation set
eval[test] = "/home/patanjali/Kaggle/Data/MS/val.libsvm"
#eval[train] = "/home/patanjali/Kaggle/Data/MS/dev.libsvm"
# The path of test data 
test:data = "/home/patanjali/Kaggle/Data/MS/test_bytes.libsvm"
#"agaricus.txt.test"      

